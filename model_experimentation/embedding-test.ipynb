{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Model Embedding Test\n",
    "\n",
    "This notebook tests the embedding capabilities of multimodal models using the provided API endpoint.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required dependencies and set up the API configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install ipykernel jupyter requests pillow numpy matplotlib seaborn scikit-learn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import base64\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# Set up API configuration\n",
    "API_BASE_URL = os.getenv(\"VLM_API\")\n",
    "API_KEY = os.getenv(\"VLM_API_KEY\")\n",
    "MODEL_NAME = \"vlm2vec-full\"\n",
    "\n",
    "# headers for API requests\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "print(f\"API Base URL: {API_BASE_URL}\")\n",
    "print(f\"API Key configured: {'Yes' if API_KEY != 'YOUR_API_KEY_HERE' else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's create helper functions for making API calls and processing embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url):\n",
    "    \"\"\"Fetches an image from a URL (wikimedia commons) and returns it as a PIL Image object.\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'MyImageLoader/1.0'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  \n",
    "        image = Image.open(io.BytesIO(response.content))\n",
    "        return image.convert('RGB')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error loading image from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def encode_image_to_base64(image_path: str) -> str:\n",
    "    \"\"\"Encode an image file to base64 string.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def encode_pil_image_to_base64_jpeg(image: Image.Image) -> str:\n",
    "    \"\"\"Encode a PIL Image to jpeg base64 string.\"\"\"\n",
    "    buffer = io.BytesIO()\n",
    "    image.convert('RGB').save(buffer, format='JPEG', quality=50, optimize=True)\n",
    "    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "def encode_pil_image_to_base64_png(image: Image.Image) -> str:\n",
    "    \"\"\"Encode a PIL Image to png base64 string.\"\"\"\n",
    "    buffer = io.BytesIO()\n",
    "    image.convert('RGB').save(buffer, format='PNG', quality=20,optimize=True)\n",
    "    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "def get_text_embedding(text: str) -> Optional[List[float]]:\n",
    "    \"\"\"\n",
    "    Get text embedding from the provided model.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"input\": text,\n",
    "        \"model\": MODEL_NAME\n",
    "    }\n",
    "        \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{API_BASE_URL}/v1/embeddings\",\n",
    "            headers=headers,\n",
    "            json=payload        \n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"data\"][0][\"embedding\"]\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"Embedding error: {e.response.status_code} - {e.response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    return None\n",
    "\n",
    "def get_image_embedding_from_url(url: str) -> Optional[List[float]]:\n",
    "    \"\"\"\n",
    "    Fetches an embedding for the image at `url` using vLLM's OpenAI-compatible /v1/embeddings API.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": url}},\n",
    "                    {\"type\": \"text\",      \"text\": \"Generate an embedding for this image.\"}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"encoding_format\": \"float\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{API_BASE_URL}/v1/embeddings\",\n",
    "            headers=headers,\n",
    "            json=payload        \n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"data\"][0][\"embedding\"]\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"Embedding error: {e.response.status_code} - {e.response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    return None\n",
    "\n",
    "def get_multimodal_embedding(image_url: str, text: str) -> Optional[List[float]]:\n",
    "    \"\"\"\n",
    "    Fetches a joint image+text embedding for the given image URL and text prompt\n",
    "    using vLLM's OpenAI-compatible /v1/embeddings API.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
    "                    {\"type\": \"text\",      \"text\": text}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"encoding_format\": \"float\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            f\"{API_BASE_URL}/v1/embeddings\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()[\"data\"][0][\"embedding\"]\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"Embedding error: {e.response.status_code} - {e.response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    return None\n",
    "\n",
    "def cosine_similarity_vectors(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def analyze_embedding(embedding: List[float], name: str = \"Embedding\") -> None:\n",
    "    \"\"\"Analyze and visualize embedding statistics.\"\"\"\n",
    "    embedding_array = np.array(embedding)\n",
    "    \n",
    "    print(f\"\\n{name} Analysis:\")\n",
    "    print(f\"  - Dimension: {len(embedding)}\")\n",
    "    print(f\"  - Mean: {embedding_array.mean():.6f}\")\n",
    "    print(f\"  - Std: {embedding_array.std():.6f}\")\n",
    "    print(f\"  - Min: {embedding_array.min():.6f}\")\n",
    "    print(f\"  - Max: {embedding_array.max():.6f}\")\n",
    "    print(f\"  - L2 Norm: {np.linalg.norm(embedding_array):.6f}\")\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(embedding_array, bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.title(f'{name} Distribution')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(embedding_array[:100])  # Plot first 100 dimensions\n",
    "    plt.title(f'{name} - First 100 Dimensions')\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Text Embeddings\n",
    "\n",
    "Let's test the text embedding capabilities with various types of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test texts for embedding\n",
    "test_texts = [\n",
    "    \"A beautiful sunset over the ocean\",\n",
    "    \"A cat sitting on a windowsill\",\n",
    "    \"The latest advances in artificial intelligence\",\n",
    "    \"A delicious pizza with melted cheese\",\n",
    "    \"A car driving on a mountain road\",\n",
    "    \"A person reading a book in a library\",\n",
    "    \"A flower garden in full bloom\",\n",
    "    \"A computer screen showing code\",\n",
    "    \"A dog playing in the park\",\n",
    "    \"A city skyline at night\"\n",
    "]\n",
    "\n",
    "# compare and contrast with two \n",
    "\n",
    "print(\"Testing text embeddings...\")\n",
    "text_embeddings = {}\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    print(f\"Processing text {i+1}/{len(test_texts)}: {text[:50]}...\")\n",
    "    embedding = get_text_embedding(text)\n",
    "    if embedding:\n",
    "        text_embeddings[text] = embedding\n",
    "        print(f\"  :) Success - Embedding dimension: {len(embedding)}\")\n",
    "    else:\n",
    "        print(f\"  :( Failed\")\n",
    "\n",
    "print(f\"\\nSuccessfully generated {len(text_embeddings)} text embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text embeddings\n",
    "if text_embeddings:\n",
    "    # Analyze first embedding\n",
    "    first_text = list(text_embeddings.keys())[0]\n",
    "    first_embedding = text_embeddings[first_text]\n",
    "    analyze_embedding(first_embedding, f\"Text: {first_text[:30]}...\")\n",
    "    \n",
    "    # Calculate similarities between all text embeddings\n",
    "    texts = list(text_embeddings.keys())\n",
    "    embeddings_matrix = np.array([text_embeddings[text] for text in texts])\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(embeddings_matrix)\n",
    "    \n",
    "    # Plot similarity matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(similarity_matrix, \n",
    "                xticklabels=[text[:20] + '...' for text in texts],\n",
    "                yticklabels=[text[:20] + '...' for text in texts],\n",
    "                cmap='viridis', \n",
    "                annot=True, \n",
    "                fmt='.3f',\n",
    "                square=True)\n",
    "    plt.title('Text Embedding Similarities')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Image Embeddings\n",
    "\n",
    "2.1) Let's test image embedding capabilities. First, we'll do a sanity check to see whether the image embedding is truly embedding the image or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_image_url = (\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/d/df/Porsche_911_GT3_Touring%2C_IAA_2017%2C_Frankfurt_%281Y7A2766%29.jpg\"\n",
    ")\n",
    "\n",
    "# 1) Embed the image via URL\n",
    "image_emb = get_image_embedding_from_url(test_image_url)\n",
    "# 2) Embed the URL string as plain text\n",
    "text_emb  = get_text_embedding(test_image_url)\n",
    "\n",
    "if image_emb is None or text_emb is None:\n",
    "    raise RuntimeError(\"Failed to retrieve one or both embeddings.\")\n",
    "\n",
    "print(f\"Embedding dimension: image={len(image_emb)}, text={len(text_emb)}\")\n",
    "\n",
    "sim = cosine_similarity_vectors(image_emb, text_emb)\n",
    "print(f\"Cosine similarity(image vs text): {sim:.6f}\")\n",
    "\n",
    "# If the model were just embedding the URL string, sim would be ≈1.0\n",
    "# Expect it to be *low* (say < 0.5) for a true image embedding\n",
    "if sim < 0.5:\n",
    "    print(f\"High similarity ({sim:.3f}) -> server may be embedding the URL text, not the image content.\")\n",
    "else:\n",
    "    print(f\"Low similarity ({sim:.3f}) -> server is embedding the image content.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2) Then, let's create some sample images for testing. You should comment out the dictionary that you don't want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs to images for url input\n",
    "sample_images = {\n",
    "    \"apple\": \"https://upload.wikimedia.org/wikipedia/commons/1/15/Red_Apple.jpg\",\n",
    "    \"banana\": \"https://upload.wikimedia.org/wikipedia/commons/8/8a/Banana-Single.jpg\",\n",
    "    \"orange\": \"https://upload.wikimedia.org/wikipedia/commons/c/c4/Orange-Fruit-Pieces.jpg\",\n",
    "    \"watermelon\": \"https://upload.wikimedia.org/wikipedia/commons/4/47/Taiwan_2009_Tainan_City_Organic_Farm_Watermelon_FRD_7962.jpg\",\n",
    "    \"peach\": \"https://upload.wikimedia.org/wikipedia/commons/9/9e/Autumn_Red_peaches.jpg\",\n",
    "    \"tennis racket\": \"https://upload.wikimedia.org/wikipedia/commons/3/3e/Tennis_Racket_and_Balls.jpg\" \n",
    "}\n",
    "\n",
    "loaded_images = {}\n",
    "for name, url in sample_images.items():\n",
    "    img = load_image_from_url(url)\n",
    "    loaded_images[name] = img\n",
    "\n",
    "# Display sample images\n",
    "fig, axes = plt.subplots(1, len(loaded_images), figsize=(15, 3))\n",
    "for ax, (name, img) in zip(axes, loaded_images.items()):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(name)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image embeddings\n",
    "print(\"Testing image embeddings...\")\n",
    "image_embeddings = {}\n",
    "\n",
    "for name, image in sample_images.items():\n",
    "    print(f\"Processing image: {name}\")\n",
    "    embedding = get_image_embedding_from_url(image)\n",
    "    if embedding:\n",
    "        image_embeddings[name] = embedding\n",
    "        print(f\"  :) Success - Embedding dimension: {len(embedding)}\")\n",
    "    else:\n",
    "        print(f\"  :( Failed\")\n",
    "\n",
    "print(f\"\\nSuccessfully generated {len(image_embeddings)} image embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image embeddings\n",
    "if image_embeddings:\n",
    "    # Analyze first embedding\n",
    "    first_image = list(image_embeddings.keys())[0]\n",
    "    first_embedding = image_embeddings[first_image]\n",
    "    analyze_embedding(first_embedding, f\"Image: {first_image}\")\n",
    "    \n",
    "    # Calculate similarities between all image embeddings\n",
    "    images = list(image_embeddings.keys())\n",
    "    embeddings_matrix = np.array([image_embeddings[img] for img in images])\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(embeddings_matrix)\n",
    "    \n",
    "    # Plot similarity matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, \n",
    "                xticklabels=images,\n",
    "                yticklabels=images,\n",
    "                cmap='viridis', \n",
    "                annot=True, \n",
    "                fmt='.3f',\n",
    "                square=True)\n",
    "    plt.title('Image Embedding Similarities')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Multimodal Embeddings\n",
    "\n",
    "Now let's test multimodal embeddings that combine text and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multimodal embeddings\n",
    "print(\"Testing multimodal embeddings...\")\n",
    "multimodal_embeddings = {}\n",
    "\n",
    "multimodal_tests_url = [\n",
    "    (\"https://upload.wikimedia.org/wikipedia/commons/f/f4/Honeycrisp.jpg\", \"A bright red apple\"),\n",
    "    (\"https://upload.wikimedia.org/wikipedia/commons/9/98/Bananas_on_black_background_02.jpg\", \"A ripe yellow banana\"),\n",
    "    (\"https://upload.wikimedia.org/wikipedia/commons/4/43/Ambersweet_oranges.jpg\", \"Some bright cut oranges\"),\n",
    "    (\"https://upload.wikimedia.org/wikipedia/commons/8/89/Citrullus_lanatus5SHSU.jpg\", \"A watermelon in a pile of leaves\"),\n",
    "    (\"https://upload.wikimedia.org/wikipedia/commons/c/cb/White_nectarine_and_cross_section02_edit.jpg\", \"A cut open juicy peach\"),\n",
    "    (\"https://upload.wikimedia.org/wikipedia/commons/3/3e/Tennis_Racket_and_Balls.jpg\", \"A tennis racket and ball on a hard court\")\n",
    "]\n",
    "\n",
    "for image, text in multimodal_tests_url:\n",
    "    print(f\"Processing multimodal: '{text}'\")\n",
    "    embedding = get_multimodal_embedding(image, text)\n",
    "    if embedding:\n",
    "        multimodal_embeddings[text] = embedding\n",
    "        print(f\"  :) Success - Embedding dimension: {len(embedding)}\")\n",
    "    else:\n",
    "        print(f\"  :( Failed\")\n",
    "\n",
    "print(f\"\\nSuccessfully generated {len(multimodal_embeddings)} multimodal embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multimodal embeddings\n",
    "if multimodal_embeddings:\n",
    "    # Analyze first embedding\n",
    "    first_multimodal = list(multimodal_embeddings.keys())[0]\n",
    "    first_embedding = multimodal_embeddings[first_multimodal]\n",
    "    analyze_embedding(first_embedding, f\"Multimodal: {first_multimodal}\")\n",
    "    \n",
    "    # Calculate similarities between all multimodal embeddings\n",
    "    multimodal_items = list(multimodal_embeddings.keys())\n",
    "    embeddings_matrix = np.array([multimodal_embeddings[item] for item in multimodal_items])\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(embeddings_matrix)\n",
    "    \n",
    "    # Plot similarity matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(similarity_matrix, \n",
    "                xticklabels=[item[:15] + '...' for item in multimodal_items],\n",
    "                yticklabels=[item[:15] + '...' for item in multimodal_items],\n",
    "                cmap='viridis', \n",
    "                annot=True, \n",
    "                fmt='.3f',\n",
    "                square=True)\n",
    "    plt.title('Multimodal Embedding Similarities')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Cross-Modal Similarity Analysis\n",
    "\n",
    "Let's analyze how similar embeddings are across different modalities (text, image, multimodal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-modal similarity analysis with comprehensive testing\n",
    "if image_embeddings and multimodal_embeddings:\n",
    "    print(\"Cross-modal similarity analysis...\")\n",
    "    print(\"Generating descriptive texts and their embeddings...\")\n",
    "    \n",
    "    # Create descriptive texts for each item\n",
    "    descriptive_texts = {\n",
    "        \"apple\": \"A juicy red fruit with a crisp texture and sweet-tart flavor, often enjoyed fresh or in pies\",\n",
    "        \"banana\": \"An elongated yellow tropical fruit with a soft, creamy texture and naturally sweet taste\",\n",
    "        \"orange\": \"A round citrus fruit with bright orange skin, juicy segments, and a tangy-sweet flavor\",\n",
    "        \"watermelon\": \"A large green striped melon with refreshing red flesh filled with black seeds and high water content\",\n",
    "        \"peach\": \"A fuzzy stone fruit with soft orange-pink flesh, sweet aromatic flavor, and a large pit in the center\",\n",
    "        \"tennis racket\": \"A tennis racket and ball on a hard court\"\n",
    "    }\n",
    "    \n",
    "    # Generate embeddings for descriptive texts\n",
    "    descriptive_embeddings = {}\n",
    "    for item_name, description in descriptive_texts.items():\n",
    "        print(f\"  Generating embedding for {item_name} description...\")\n",
    "        embedding = get_text_embedding(description)\n",
    "        if embedding:\n",
    "            descriptive_embeddings[item_name] = embedding\n",
    "            print(f\"    ✓ Success\")\n",
    "        else:\n",
    "            print(f\"    ✗ Failed\")\n",
    "    \n",
    "    # Map image names to their corresponding multimodal texts\n",
    "    image_to_multimodal = {\n",
    "        \"apple\": \"A bright red apple\",\n",
    "        \"banana\": \"A ripe yellow banana\",\n",
    "        \"orange\": \"Some bright cut oranges\",\n",
    "        \"watermelon\": \"A watermelon in a pile of leaves\",\n",
    "        \"peach\": \"A cut open juicy peach\",\n",
    "        \"tennis racket\": \"A tennis racket and ball on a hard court\"\n",
    "    }\n",
    "    \n",
    "    # Collect all available items\n",
    "    all_items = []\n",
    "    for item_name in descriptive_embeddings.keys():\n",
    "        if (item_name in image_embeddings and \n",
    "            item_name in image_to_multimodal and \n",
    "            image_to_multimodal[item_name] in multimodal_embeddings):\n",
    "            \n",
    "            all_items.append({\n",
    "                'name': item_name,\n",
    "                'descriptive_text': descriptive_texts[item_name],\n",
    "                'multimodal_text': image_to_multimodal[item_name],\n",
    "                'desc_emb': descriptive_embeddings[item_name],\n",
    "                'image_emb': image_embeddings[item_name],\n",
    "                'multimodal_emb': multimodal_embeddings[image_to_multimodal[item_name]]\n",
    "            })\n",
    "    \n",
    "    if all_items:\n",
    "        print(f\"\\nFound {len(all_items)} items for comprehensive analysis\")\n",
    "        \n",
    "        # 1. SELF-SIMILARITY ANALYSIS (same item across modalities)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SELF-SIMILARITY ANALYSIS (same item across modalities)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self_results = []\n",
    "        for item in all_items:\n",
    "            desc_img_sim = cosine_similarity_vectors(item['desc_emb'], item['image_emb'])\n",
    "            desc_multi_sim = cosine_similarity_vectors(item['desc_emb'], item['multimodal_emb'])\n",
    "            img_multi_sim = cosine_similarity_vectors(item['image_emb'], item['multimodal_emb'])\n",
    "            \n",
    "            self_results.append({\n",
    "                'name': item['name'],\n",
    "                'desc_img': desc_img_sim,\n",
    "                'desc_multimodal': desc_multi_sim,\n",
    "                'img_multimodal': img_multi_sim\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n{item['name'].upper()}:\")\n",
    "            print(f\"  Descriptive Text ↔ Image:      {desc_img_sim:.4f}\")\n",
    "            print(f\"  Descriptive Text ↔ Multimodal: {desc_multi_sim:.4f}\")\n",
    "            print(f\"  Image ↔ Multimodal:            {img_multi_sim:.4f}\")\n",
    "        \n",
    "        # 2. CROSS-ITEM SIMILARITY ANALYSIS\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CROSS-ITEM SIMILARITY ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create similarity matrices for each modality pair\n",
    "        item_names = [item['name'] for item in all_items]\n",
    "        n_items = len(all_items)\n",
    "        \n",
    "        # Initialize similarity matrices\n",
    "        img_to_img_matrix = np.zeros((n_items, n_items))\n",
    "        desc_to_desc_matrix = np.zeros((n_items, n_items))\n",
    "        multi_to_multi_matrix = np.zeros((n_items, n_items))\n",
    "        img_to_multi_matrix = np.zeros((n_items, n_items))\n",
    "        desc_to_img_matrix = np.zeros((n_items, n_items))\n",
    "        desc_to_multi_matrix = np.zeros((n_items, n_items))\n",
    "        \n",
    "        # Calculate all pairwise similarities\n",
    "        for i, item1 in enumerate(all_items):\n",
    "            for j, item2 in enumerate(all_items):\n",
    "                img_to_img_matrix[i, j] = cosine_similarity_vectors(item1['image_emb'], item2['image_emb'])\n",
    "                desc_to_desc_matrix[i, j] = cosine_similarity_vectors(item1['desc_emb'], item2['desc_emb'])\n",
    "                multi_to_multi_matrix[i, j] = cosine_similarity_vectors(item1['multimodal_emb'], item2['multimodal_emb'])\n",
    "                img_to_multi_matrix[i, j] = cosine_similarity_vectors(item1['image_emb'], item2['multimodal_emb'])\n",
    "                desc_to_img_matrix[i, j] = cosine_similarity_vectors(item1['desc_emb'], item2['image_emb'])\n",
    "                desc_to_multi_matrix[i, j] = cosine_similarity_vectors(item1['desc_emb'], item2['multimodal_emb'])\n",
    "        \n",
    "        # Visualize similarity matrices\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Cross-Item Similarity Matrices', fontsize=16)\n",
    "        \n",
    "        matrices = [\n",
    "            (img_to_img_matrix, 'Image -> Image'),\n",
    "            (desc_to_desc_matrix, 'Desc -> Desc'),\n",
    "            (multi_to_multi_matrix, 'Multi -> Multi'),\n",
    "            (img_to_multi_matrix, 'Image -> Multi'),\n",
    "            (desc_to_img_matrix, 'Desc -> Image'),\n",
    "            (desc_to_multi_matrix, 'Desc -> Multi')\n",
    "        ]\n",
    "        \n",
    "        for idx, (matrix, title) in enumerate(matrices):\n",
    "            ax = axes[idx // 3, idx % 3]\n",
    "            im = ax.imshow(matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "            ax.set_title(title)\n",
    "            ax.set_xticks(np.arange(n_items))\n",
    "            ax.set_yticks(np.arange(n_items))\n",
    "            ax.set_xticklabels(item_names, rotation=45, ha='right')\n",
    "            ax.set_yticklabels(item_names)\n",
    "            \n",
    "            # Add text annotations for values\n",
    "            for i in range(n_items):\n",
    "                for j in range(n_items):\n",
    "                    text = ax.text(j, i, f'{matrix[i, j]:.2f}',\n",
    "                                 ha=\"center\", va=\"center\", \n",
    "                                 color=\"white\" if matrix[i, j] < 0.5 else \"black\",\n",
    "                                 fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 3. TENNIS RACKET vs FRUITS ANALYSIS\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TENNIS RACKET vs FRUITS ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if 'tennis racket' in item_names:\n",
    "            tennis_idx = item_names.index('tennis racket')\n",
    "            tennis_item = all_items[tennis_idx]\n",
    "            \n",
    "            print(\"\\nComparing tennis racket with each fruit:\")\n",
    "            fruit_comparisons = []\n",
    "            \n",
    "            for item in all_items:\n",
    "                if item['name'] != 'tennis racket':\n",
    "                    # Calculate all cross-modal similarities\n",
    "                    tennis_img_to_fruit_img = cosine_similarity_vectors(tennis_item['image_emb'], item['image_emb'])\n",
    "                    tennis_img_to_fruit_multi = cosine_similarity_vectors(tennis_item['image_emb'], item['multimodal_emb'])\n",
    "                    tennis_multi_to_fruit_img = cosine_similarity_vectors(tennis_item['multimodal_emb'], item['image_emb'])\n",
    "                    tennis_multi_to_fruit_multi = cosine_similarity_vectors(tennis_item['multimodal_emb'], item['multimodal_emb'])\n",
    "                    tennis_desc_to_fruit_desc = cosine_similarity_vectors(tennis_item['desc_emb'], item['desc_emb'])\n",
    "                    \n",
    "                    fruit_comparisons.append({\n",
    "                        'fruit': item['name'],\n",
    "                        'img_to_img': tennis_img_to_fruit_img,\n",
    "                        'img_to_multi': tennis_img_to_fruit_multi,\n",
    "                        'multi_to_img': tennis_multi_to_fruit_img,\n",
    "                        'multi_to_multi': tennis_multi_to_fruit_multi,\n",
    "                        'desc_to_desc': tennis_desc_to_fruit_desc\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"\\n  Tennis Racket -> {item['name'].capitalize()}:\")\n",
    "                    print(f\"    Image -> Image:           {tennis_img_to_fruit_img:.4f}\")\n",
    "                    print(f\"    Image -> Multimodal:      {tennis_img_to_fruit_multi:.4f}\")\n",
    "                    print(f\"    Multimodal -> Image:      {tennis_multi_to_fruit_img:.4f}\")\n",
    "                    print(f\"    Multimodal -> Multimodal: {tennis_multi_to_fruit_multi:.4f}\")\n",
    "                    print(f\"    Desc -> Desc:             {tennis_desc_to_fruit_desc:.4f}\")\n",
    "            \n",
    "            # Visualize tennis racket comparisons with bar chart only\n",
    "            if fruit_comparisons:\n",
    "                fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                \n",
    "                # Bar chart of tennis racket similarities\n",
    "                fruits = [comp['fruit'] for comp in fruit_comparisons]\n",
    "                x = np.arange(len(fruits))\n",
    "                width = 0.15\n",
    "                \n",
    "                metrics = ['img_to_img', 'img_to_multi', 'multi_to_img', 'multi_to_multi', 'desc_to_desc']\n",
    "                colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "                \n",
    "                for i, metric in enumerate(metrics):\n",
    "                    values = [comp[metric] for comp in fruit_comparisons]\n",
    "                    ax.bar(x + i * width - 2 * width, values, width, \n",
    "                           label=metric.replace('_', ' -> ').replace('to', '->').title(),\n",
    "                           color=colors[i])\n",
    "                \n",
    "                ax.set_xlabel('Fruit')\n",
    "                ax.set_ylabel('Cosine Similarity')\n",
    "                ax.set_title('Tennis Racket Similarity to Fruits (All Modalities)')\n",
    "                ax.set_xticks(x)\n",
    "                ax.set_xticklabels(fruits)\n",
    "                ax.legend(loc='upper right')\n",
    "                ax.grid(axis='y', alpha=0.3)\n",
    "                ax.set_ylim(0, 0.8)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        # 4. SUMMARY STATISTICS\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Find most and least similar pairs across modalities\n",
    "        print(\"\\nMost Similar Cross-Item Pairs (excluding self-similarities):\")\n",
    "        cross_similarities = []\n",
    "        for i, item1 in enumerate(all_items):\n",
    "            for j, item2 in enumerate(all_items):\n",
    "                if i != j:  # Exclude self-similarities\n",
    "                    cross_similarities.append({\n",
    "                        'pair': f\"{item1['name']} -> {item2['name']}\",\n",
    "                        'img_to_img': img_to_img_matrix[i, j],\n",
    "                        'multi_to_multi': multi_to_multi_matrix[i, j],\n",
    "                        'img_to_multi': img_to_multi_matrix[i, j]\n",
    "                    })\n",
    "        \n",
    "        # Sort by different metrics\n",
    "        for metric in ['img_to_img', 'multi_to_multi', 'img_to_multi']:\n",
    "            sorted_pairs = sorted(cross_similarities, key=lambda x: x[metric], reverse=True)\n",
    "            print(f\"\\n  Top 3 by {metric}:\")\n",
    "            for i, pair in enumerate(sorted_pairs[:3]):\n",
    "                print(f\"    {i+1}. {pair['pair']}: {pair[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Semantic Search Demo\n",
    "\n",
    "Let's demonstrate semantic search capabilities using the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query_embedding: List[float], \n",
    "                    candidate_embeddings: Dict[str, List[float]], \n",
    "                    top_k: int = 3) -> List[tuple]:\n",
    "    \"\"\"Perform semantic search using cosine similarity.\"\"\"\n",
    "    similarities = []\n",
    "    \n",
    "    for name, embedding in candidate_embeddings.items():\n",
    "        similarity = cosine_similarity_vectors(query_embedding, embedding)\n",
    "        similarities.append((name, similarity))\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Create multimodal embeddings (text + image combinations)\n",
    "print(\"Creating multimodal embeddings...\")\n",
    "multimodal_embeddings = {}\n",
    "\n",
    "# Define text descriptions for each image\n",
    "image_descriptions = {\n",
    "    \"apple\": \"A fresh red apple fruit\",\n",
    "    \"banana\": \"A yellow banana fruit\", \n",
    "    \"orange\": \"An orange citrus fruit\",\n",
    "    \"watermelon\": \"A large green watermelon\",\n",
    "    \"peach\": \"Autumn red peaches\",\n",
    "    \"tennis racket\": \"Tennis racket with balls\"\n",
    "}\n",
    "\n",
    "# Generate multimodal embeddings\n",
    "for name, image_url in sample_images.items():\n",
    "    if name in image_descriptions:\n",
    "        print(f\"Processing {name}...\")\n",
    "        description = image_descriptions[name]\n",
    "        embedding = get_multimodal_embedding(image_url, description)\n",
    "        if embedding:\n",
    "            multimodal_embeddings[f\"{description} | {name}\"] = embedding\n",
    "            print(f\"  :( Success\")\n",
    "        else:\n",
    "            print(f\"  :) Failed\")\n",
    "\n",
    "print(f\"\\nGenerated {len(multimodal_embeddings)} multimodal embeddings\")\n",
    "\n",
    "# Test multimodal semantic search\n",
    "if multimodal_embeddings:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Multimodal Semantic Search Demo\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test with text + image queries\n",
    "    test_queries = [\n",
    "        {\n",
    "            \"text\": \"healthy snack for athletes\",\n",
    "            \"image\": sample_images[\"apple\"]  # Apple as reference image\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"tropical fruit that's yellow\", \n",
    "            \"image\": sample_images[\"banana\"]  # Banana as reference\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"sports equipment for outdoor games\",\n",
    "            \"image\": sample_images[\"tennis racket\"]  # Tennis racket as reference\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"juicy summer fruit\",\n",
    "            \"image\": sample_images[\"watermelon\"]  # Watermelon as reference\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nQuery {i}: '{query['text']}'\")\n",
    "        print(f\"Reference image: {query['image'].split('/')[-1]}\")\n",
    "        \n",
    "        # Generate multimodal query embedding\n",
    "        query_embedding = get_multimodal_embedding(query['image'], query['text'])\n",
    "        \n",
    "        if query_embedding:\n",
    "            results = semantic_search(query_embedding, multimodal_embeddings, top_k=5)\n",
    "            print(\"Top matches:\")\n",
    "            for j, (name, similarity) in enumerate(results, 1):\n",
    "                print(f\"  {j}. {name} (similarity: {similarity:.4f})\")\n",
    "        else:\n",
    "            print(\"Failed to generate query embedding\")\n",
    "    \n",
    "    # Cross-modal search: Text-only query against multimodal embeddings\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Cross-modal Search (Text -> Multimodal)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    text_only_queries = [\n",
    "        \"vitamin C rich citrus\",\n",
    "        \"tennis and sports\",\n",
    "        \"sweet red fruit\",\n",
    "        \"large green fruit with seeds\"\n",
    "    ]\n",
    "    \n",
    "    for query_text in text_only_queries:\n",
    "        print(f\"\\nText query: '{query_text}'\")\n",
    "        \n",
    "        # Use a neutral/placeholder image for text-only queries\n",
    "        # Or you could create a separate text-only embedding function\n",
    "        query_embedding = get_text_embedding(query_text)\n",
    "        \n",
    "        if query_embedding:\n",
    "            results = semantic_search(query_embedding, multimodal_embeddings, top_k=3)\n",
    "            print(\"Top matches:\")\n",
    "            for j, (name, similarity) in enumerate(results, 1):\n",
    "                print(f\"  {j}. {name} (similarity: {similarity:.4f})\")\n",
    "        else:\n",
    "            print(\"Failed to generate query embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Embedding Quality Analysis\n",
    "\n",
    "Let's analyze the quality and characteristics of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_embedding_quality(embeddings_dict: Dict[str, List[float]], \n",
    "                             name: str = \"Embeddings\") -> None:\n",
    "    \"\"\"Analyze the quality and characteristics of embeddings.\"\"\"\n",
    "    if not embeddings_dict:\n",
    "        print(f\"No {name.lower()} available for analysis\")\n",
    "        return\n",
    "    \n",
    "    embeddings_list = list(embeddings_dict.values())\n",
    "    embeddings_array = np.array(embeddings_list)\n",
    "    \n",
    "    print(f\"\\n{name} Quality Analysis:\")\n",
    "    print(f\"  - Number of embeddings: {len(embeddings_list)}\")\n",
    "    print(f\"  - Embedding dimension: {embeddings_array.shape[1]}\")\n",
    "    print(f\"  - Mean embedding norm: {np.mean([np.linalg.norm(emb) for emb in embeddings_list]):.6f}\")\n",
    "    print(f\"  - Std embedding norm: {np.std([np.linalg.norm(emb) for emb in embeddings_list]):.6f}\")\n",
    "    \n",
    "    # Calculate pairwise similarities\n",
    "    similarity_matrix = cosine_similarity(embeddings_array)\n",
    "    \n",
    "    # Remove diagonal (self-similarities)\n",
    "    mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)\n",
    "    off_diagonal_similarities = similarity_matrix[mask]\n",
    "    \n",
    "    print(f\"  - Mean pairwise similarity: {off_diagonal_similarities.mean():.6f}\")\n",
    "    print(f\"  - Std pairwise similarity: {off_diagonal_similarities.std():.6f}\")\n",
    "    print(f\"  - Min pairwise similarity: {off_diagonal_similarities.min():.6f}\")\n",
    "    print(f\"  - Max pairwise similarity: {off_diagonal_similarities.max():.6f}\")\n",
    "    \n",
    "    # Plot similarity distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(off_diagonal_similarities, bins=30, alpha=0.7, edgecolor='black')\n",
    "    plt.title(f'{name} - Pairwise Similarity Distribution')\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(similarity_matrix, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='Cosine Similarity')\n",
    "    plt.title(f'{name} - Similarity Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze quality for each modality\n",
    "analyze_embedding_quality(text_embeddings, \"Text Embeddings\")\n",
    "analyze_embedding_quality(image_embeddings, \"Image Embeddings\")\n",
    "analyze_embedding_quality(multimodal_embeddings, \"Multimodal Embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
